---
layout: post
title: Communication Method, Scale, and Entropy
date: 2013-03-27 05:20:31.000000000 -04:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Research
tags:
- networks
- scale
meta:
  _edit_last: '2'
  rcp_user_level: None
  _wpas_done_all: '1'
  _wp_rp_image: '1002'
  _wp_rp_related_posts_query_result_cache_expiration: '1556683389'
  _wp_rp_related_posts_query_result_cache_6: a:12:{i:0;O:8:"stdClass":2:{s:7:"post_id";s:3:"277";s:5:"score";s:17:"32.71279117833487";}i:1;O:8:"stdClass":2:{s:7:"post_id";s:3:"260";s:5:"score";s:17:"32.71279117833487";}i:2;O:8:"stdClass":2:{s:7:"post_id";s:1:"6";s:5:"score";s:17:"32.71279117833487";}i:3;O:8:"stdClass":2:{s:7:"post_id";s:3:"811";s:5:"score";s:18:"29.092752182205853";}i:4;O:8:"stdClass":2:{s:7:"post_id";s:3:"985";s:5:"score";s:18:"23.061058576953293";}i:5;O:8:"stdClass":2:{s:7:"post_id";s:4:"1254";s:5:"score";s:18:"22.136154497945952";}i:6;O:8:"stdClass":2:{s:7:"post_id";s:3:"895";s:5:"score";s:17:"21.19058439599707";}i:7;O:8:"stdClass":2:{s:7:"post_id";s:3:"599";s:5:"score";s:17:"19.88930219950142";}i:8;O:8:"stdClass":2:{s:7:"post_id";s:3:"857";s:5:"score";s:18:"19.763884620329733";}i:9;O:8:"stdClass":2:{s:7:"post_id";s:4:"1181";s:5:"score";s:17:"17.84442466050548";}i:10;O:8:"stdClass":2:{s:7:"post_id";s:4:"1100";s:5:"score";s:18:"13.941451933490129";}i:11;O:8:"stdClass":2:{s:7:"post_id";s:3:"971";s:5:"score";s:18:"13.941451933490129";}}
  _jetpack_related_posts_cache: a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554280827;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1510;}i:1;a:1:{s:2:"id";i:895;}i:2;a:1:{s:2:"id";i:857;}}}}
author:
  login: erich
  email: erich@howweknowus.com
  display_name: erich
  first_name: Erich
  last_name: Morisse
permalink: "/communication-method-scale-and-entropy/"
excerpt: "\n\t\t\t\t\t\t"
---
<p>
				In a surprise to Marshall McLuhan, we see <em>ad hoc</em> conversations conducted through different electronic media demonstrating very similar scaling characteristics across number of nodes, number of edges, and number of unique edges.  Looking at email lists, IRC, and long-term twitter searches, we more similarity than difference between the three media.</p>
<p>However, when look at the observed conditional entropy (below the fold), the differences become clear: communication patterns are very different by media type, even as the networks scale similarly in communicants.  Maybe McLuhan was right.</p>
<p><a href="http://www.howweknowus.com/wp-content/uploads/2013/03/conditionalslog.png"><img class=" wp-image-1002   alignleft" alt="nodes vs unique edges" src="{{ site.baseurl }}/assets/conditionalslog.png" width="173" height="173" /></a></p>
<p><a href="http://www.howweknowus.com/wp-content/uploads/2013/03/nodesVtotal.png"><img class="wp-image-1001  alignleft" alt="nodes vs total edges" src="{{ site.baseurl }}/assets/nodesVtotal.png" width="173" height="173" /></a></p>
<p><a href="http://www.howweknowus.com/wp-content/uploads/2013/03/edgesVtotal.png"><img class="wp-image-1000  alignleft" alt="unique vs total edges" src="{{ site.baseurl }}/assets/edgesVtotal.png" width="173" height="173" /></a></p>
<p>&nbsp;</p>
<p><!--more--></p>
<p>Wikipedia defines <a href="http://en.wikipedia.org/wiki/Conditional_entropy">conditional entropy</a> as: the amount of information needed to describe the outcome of a random variable <strong>Y</strong> given that the value of another random variable <strong>X</strong> is known. For our networks, <strong>X </strong>is the source node, and <strong>Y</strong> the recipient. We sum these across all source nodes, and we have the conditional entropy on the observed network. This number is strongly affected by "balance" of communication, and reaches is maximum value when all nodes communicate with all other nodes the same amount.</p>
<p>Comparing the observed conditional entropy with maximum theoretical entropy for the same network, we see the first strong divergence in characteristics. The size of the points in the below graphs (as well as the above) are the ratio of observed to maximum conditional entropy, or relative conditional entropy. The larger the ratio, the greater balance in communication between all participants. Playing loosely with terms, very democratic networks have large ratios, and despotic small.</p>
<p><a href="http://www.howweknowus.com/wp-content/uploads/2013/03/entropy.png"><img class="size-full wp-image-999 alignnone" alt="entropy" src="{{ site.baseurl }}/assets/entropy.png" width="480" height="480" /></a></p>
<p>&nbsp;</p>
<p>Below, we can see the observed conditional entropy for each sample by type. The box indicates the 95% confidence interval for the type in question. We see a strong distinction between IRC and twitter, with email showing much more loose clustering.<br />
<a href="http://www.howweknowus.com/wp-content/uploads/2013/03/entropyvar.png"><img class="size-full wp-image-1011 alignnone" alt="entropyvar" src="{{ site.baseurl }}/assets/entropyvar.png" width="480" height="480" /></a> <!--more--></p>
<h3>Comments on methodology</h3>
<p><strong>Nodes</strong> are individuals whom have communicated to another node, and/or have been on the receiving end of a communication.  Membership to email lists and attendance in an IRC channel are not included, nor are broadcast messages without individual designated recipients.</p>
<p>All <strong>edges</strong> are directed for the calculation of observed entropy.</p>
<p>Both <strong>email</strong> and <strong>IRC</strong> sources are technical in content.  Future study recommends observations across a wide-set of content types.</p>
<p><strong>Twitter</strong> search topics include a wide range including technical, commercial brand, and political terms.</p>
<p><!--more--></p>
<h3>R code to generate the above graphs</h3>
<p><code> library(ggplot2)<br />
csv2&lt;-read.csv("conditionalentropies.csv",header=T)<br />
csv2$edges &lt;- csv$lines # for clarity in graph legends<br />
attach(csv2)<br />
png("conditionalslog.png")<br />
qplot(log(nodes), log(edges), data=csv2,colour=type, cex=ratio) + geom_smooth(method = 'lm', alpha = 0.2) dev.off()<br />
png("nodesVtotal.png")<br />
qplot(log(nodes), log(total/2), data=csv2,colour=type, cex=ratio) + geom_smooth(method = 'lm', alpha = 0.2)<br />
dev.off()<br />
png("edgesVtotal.png")<br />
qplot(log(lines), log(total/2), data=csv2,colour=type, cex=ratio) + geom_smooth(method = 'lm', alpha = 0.2)<br />
dev.off()<br />
png("entropy.png")<br />
qplot(max, entropy, data=csv2,colour=type, cex=ratio) + geom_smooth(method = 'lm', alpha = 0.2)<br />
dev.off()<br />
library(Hmisc)<br />
png("entropyvar.png")<br />
smry &lt;- stat_summary(fun.data="mean_cl_boot", conf.int=0.95, geom="crossbar", width=0.3)<br />
qplot(type, entropy,col=type) + smry<br />
dev.off()<br />
</code></p>
<h3>Notes on the code</h3>
<p><strong></strong>"lines" and "edges" are used interchangeably, these are the same value.</p>
<p>"total/2" is the total number of observed edges, not corrected for uniqueness.  E.g. edges seen in the following pattern: A-B A-B C-D, would have a total/2 of 3. The "/2" is in place to correct an artifact of the way the data was collected.</p>
<p>Graph formatting ideas from a <a href="http://ggplot2.org/resources/2007-past-present-future.pdf">presentation</a> by <a href="http://had.co.nz/ ">Hadley Wickham</a>.		</p>
